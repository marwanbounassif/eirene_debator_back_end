{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786cd1a5",
   "metadata": {},
   "source": [
    "## Helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7462a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_display_logs(log_path):\n",
    "    \"\"\"Reads and displays the content of the log file.\"\"\"\n",
    "    import os\n",
    "\n",
    "    if os.path.exists(log_path):\n",
    "        print(f\"\\nReading logs from: {log_path}\\n\")\n",
    "        with open(log_path, \"r\") as log_file:\n",
    "            logs = log_file.read()\n",
    "            print(logs)\n",
    "    else:\n",
    "        print(f\"Log file does not exist: {log_path}\")\n",
    "\n",
    "# Example usage\n",
    "log_path = \"work/app/logs/app.log\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb03b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5-search-api\n",
      "gpt-5-search-api-2025-10-14\n",
      "dall-e-2\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-audio\n",
      "gpt-4o-mini-tts\n",
      "gpt-4-turbo\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-4-0613\n",
      "gpt-4\n",
      "whisper-1\n",
      "gpt-4.1\n",
      "gpt-4.1-2025-04-14\n",
      "text-embedding-ada-002\n",
      "dall-e-3\n",
      "gpt-4.1-nano\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "o1-2024-12-17\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-audio-2025-08-28\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "omni-moderation-latest\n",
      "o1-pro\n",
      "o1-pro-2025-03-19\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "text-embedding-3-small\n",
      "o1-mini-2024-09-12\n",
      "gpt-4o-2024-08-06\n",
      "o1\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-image-1-mini\n",
      "gpt-5-mini\n",
      "gpt-image-1\n",
      "gpt-5-mini-2025-08-07\n",
      "omni-moderation-2024-09-26\n",
      "gpt-5\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-5-nano\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "tts-1-hd\n",
      "tts-1\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o3-mini-2025-01-31\n",
      "o3-mini\n",
      "o1-mini\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4-1106-preview\n",
      "babbage-002\n",
      "gpt-5-chat-latest\n",
      "gpt-3.5-turbo\n",
      "gpt-5-2025-08-07\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-transcribe\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-pro-2025-10-06\n",
      "davinci-002\n",
      "gpt-4o\n",
      "gpt-4o-realtime-preview\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "o4-mini\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "o4-mini-2025-04-16\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "gpt-5-pro\n",
      "text-embedding-3-large\n",
      "gpt-4o-mini-transcribe\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "o3\n",
      "o3-2025-04-16\n",
      "gpt-4o-transcribe-diarize\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-5-codex\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "models = openai.models.list()\n",
    "for model in models.data:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0211da",
   "metadata": {},
   "source": [
    "### Full Debate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a01750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading CHARACTER_DUMP_PATH:\n",
      "- bd91656ec9ae.json\n",
      "- e21b1dba641c.json\n",
      "- 0af0871192d0.json\n",
      "- 70e046e7d721.json\n",
      "\n",
      "Reading DEBATE_CONFIG_PATH:\n",
      "{\n",
      "    \"character_creation_prompt\": [\n",
      "        \"You are a character creation assistant for app where LLMs will debate eachother\",\n",
      "        \"You will be provided a description of a character provided by a user\",\n",
      "        \"You must generate a json string with the following format: {'name': 'name': '', 'debate_style': '', 'personality_description': '', 'extra_details': ''}\",\n",
      "        \"Do not generate any other text other than the json string it must be parasable directly in python\",\n",
      "        \"You may not be provided all details directly, you must infer them based on what a user inputs, do not make major changes\",\n",
      "        \"If your character is a known figure search the internet or fill in the json with known information of the character, still respecting other inputs from the user\",\n",
      "        \"Your outputs will be input into another LLM please format all responses accorindly\",\n",
      "        \"You may paraphrase and condense information where needed but do not change the meaning of the users input\",\n",
      "        \"In all subsections write a single string, do not worry about string length\",\n",
      "        \"In the extra_details section give all extra information that may not fit into the other categories as well as clear instructions on how this person should behave during a debate, this can include speach patterns, manurisims, vocabulary, language proficiency, and more. Write at least 100 words in this section.\"\n",
      "    ],\n",
      "    \"opening_statement_prompt\": \"You will now debate your opponent. Please provide your opening statements of around 40 words on the following topic:\",\n",
      "    \"closing_statement_prompt\": \"This is the last round of debating. Pleased provide your closing statements. Limit your response to around 40 words\",\n",
      "    \"rebutal_prompt\": \"Please expand on your previously stated ideas and/or respond to the comments of your opponent and/or state a new point\",\n",
      "    \"debate_rounds_count\": 5,\n",
      "    \"context_response_prompt\": \"You have been provided a character description of yourself. You will debate on an oppentent in a set number of rounds. State your name at the start of every response. You may also recieve extra context based on the state of the debate. Limit your response to around 40 words. Respond accorindly.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read from volumes: CHARACTER_DUMP_PATH and DEBATE_CONFIG_PATH\n",
    "import json\n",
    "\n",
    "# Get paths from environment variables\n",
    "character_dump_path = \"/home/jovyan/work/characters\"\n",
    "debate_config_path = \"/home/jovyan/work/configs/debate_config.json\"\n",
    "\n",
    "# Read and print character dump files\n",
    "print(\"\\nReading CHARACTER_DUMP_PATH:\")\n",
    "if os.path.exists(character_dump_path):\n",
    "    for file_name in os.listdir(character_dump_path):\n",
    "        file_path = os.path.join(character_dump_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"- {file_name}\")\n",
    "else:\n",
    "    print(f\"Path does not exist: {character_dump_path}\")\n",
    "\n",
    "# Read and print debate config file\n",
    "print(\"\\nReading DEBATE_CONFIG_PATH:\")\n",
    "if os.path.exists(debate_config_path):\n",
    "    with open(debate_config_path, \"r\") as f:\n",
    "        debate_config = json.load(f)\n",
    "        print(json.dumps(debate_config, indent=4))\n",
    "else:\n",
    "    print(f\"Path does not exist: {debate_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff6ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install langgraph langchain langchain-openai python-dotenv\n",
    "\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Sequence, Literal\n",
    "from langchain_core.messages import BaseMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import operator\n",
    "\n",
    "# or\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005239f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3810b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-3.5-turbo-1106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dec7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the debate state\n",
    "class DebateState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    turn_count: int\n",
    "    max_turns: int\n",
    "    current_speaker: str\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=model_name, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8286f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_debater(name: str, personality: str, topic: str):\n",
    "    \"\"\"Create a debater function with specific personality\"\"\"\n",
    "    \n",
    "    def debate_turn(state: DebateState):\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"\"\"You are {name}.\n",
    "            Personality: {personality}\n",
    "            Topic: {topic}\n",
    "            \n",
    "            Make a concise argument (2-3 sentences).\n",
    "            Respond to previous points if any.\n",
    "            Stay in character.\"\"\"),\n",
    "            *state[\"messages\"]\n",
    "        ]\n",
    "        \n",
    "        response = llm.invoke(messages)\n",
    "        print(f\"\\n{name}: {response.content}\")\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"{name}: {response.content}\")],\n",
    "            \"turn_count\": state[\"turn_count\"] + 1\n",
    "        }\n",
    "    \n",
    "    return debate_turn\n",
    "\n",
    "def should_continue(state: DebateState) -> Literal[\"continue\", \"end\"]:\n",
    "    \"\"\"Check if debate should continue\"\"\"\n",
    "    return \"end\" if state[\"turn_count\"] >= state[\"max_turns\"] else \"continue\"\n",
    "\n",
    "def judge_summary(state: DebateState):\n",
    "    \"\"\"Judge provides final verdict\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\"\"You are an impartial judge. \n",
    "        Provide a brief verdict on who won this debate and why.\n",
    "        Keep it to 3-4 sentences.\"\"\"),\n",
    "        *state[\"messages\"]\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    print(f\"\\nðŸ”¨ VERDICT: {response.content}\")\n",
    "    \n",
    "    return {\"messages\": [AIMessage(content=f\"Judge: {response.content}\")]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c89735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load character data\n",
    "characters = []\n",
    "if os.path.exists(character_dump_path):\n",
    "    for file_name in os.listdir(character_dump_path):\n",
    "        file_path = os.path.join(character_dump_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                character_data = json.load(f)\n",
    "                characters.append(character_data)\n",
    "else:\n",
    "    print(f\"Path does not exist: {character_dump_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368beb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = characters[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "997080e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Ahmad El Sharaa',\n",
       "  'debate_style': 'Diplomatic and assertive, focusing on finding common ground through respectful dialogue.',\n",
       "  'personality_description': 'A former Syrian politician and diplomat, known for his experience in international relations and conflict resolution.',\n",
       "  'extra_details': 'Ahmad El Sharaa was a Syrian politician who served as the Deputy Prime Minister from 1988 to 1992 and later as the Foreign Minister from 1992 to 1994. He played a crucial role in the Syrian peace negotiations with Israel in the 1990s. As a diplomat, he is known for his calm and composed demeanor, often adopting a conciliatory tone in negotiations. During debates, he is expected to maintain a respectful and professional attitude, actively listening to opposing viewpoints and seeking to understand the underlying concerns. He should employ a clear and concise speaking style, using formal language and avoiding aggressive tone. His vocabulary should reflect his experience in international relations, incorporating relevant terms and concepts. When engaging with counterarguments, he should remain composed, acknowledging points made by opponents and responding thoughtfully. El Sharaa is fluent in Arabic, English, and French, which should be utilized during the debate to provide context and support his arguments. He should also be prepared to draw upon his knowledge of international relations, history, and politics to support his points, using real-world examples to illustrate his arguments. Throughout the debate, he should maintain eye contact, using non-verbal cues to engage with the audience and demonstrate his confidence in his arguments.'},\n",
       " {'name': 'Donald Trump',\n",
       "  'debate_style': 'Aggressive, confrontational, and argumentative, with a strong emphasis on personal attacks and criticism of opponents.',\n",
       "  'personality_description': 'Confident, charismatic, and assertive, with a tendency to dominate conversations and disregard opposing views.',\n",
       "  'extra_details': \"Donald Trump has a distinctive speaking style that is characterized by the use of simple, short sentences and an emphasis on repetition. He often uses phrases such as 'believe me' and 'it's gonna be huge' to emphasize his points. In a debate, he may use body language such as pointing at his opponents, scowling, and making dismissive gestures. He has a tendency to interrupt and talk over others, and may use inflammatory language or personal attacks to deflect criticism. He is highly proficient in English, with a strong vocabulary and a talent for using language to create a sense of excitement and drama. However, he may struggle with more complex or nuanced ideas, and may rely on emotional appeals rather than logical reasoning to make his points. It's also worth noting that Trump is known for his use of social media, and may try to use this to his advantage in a debate by trying to get his opponents to engage with him on these platforms. Instructions for simulating Donald Trump in a debate: Use a confident, assertive tone of voice, with a tendency to raise the volume and pace when arguing a point. Use simple, short sentences and an emphasis on repetition to drive home key messages. Use body language such as pointing, scowling, and making dismissive gestures to emphasize points. Interrupt and talk over opponents when necessary, but avoid coming across as too aggressive or confrontational. Try to use inflammatory language or personal attacks to deflect criticism, but avoid crossing the line into outright abuse. Use social media to engage with opponents and try to create a sense of drama and excitement around the debate.\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c1674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_debater.<locals>.debate_turn(state: __main__.DebateState)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18b19dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the debate parameters\n",
    "\n",
    "\n",
    "topic = \"Should AI development be accelerated or slowed down?\"\n",
    "\n",
    "agent1 = create_debater(\n",
    "    name=characters[0][\"name\"],\n",
    "    personality=characters[0][\"personality_description\"],\n",
    "    topic=topic\n",
    ")\n",
    "\n",
    "agent2 = create_debater(\n",
    "    name=characters[1][\"name\"],\n",
    "    personality=characters[1][\"personality_description\"],\n",
    "    topic=topic\n",
    ")\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(DebateState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent1\", agent1)\n",
    "workflow.add_node(\"agent2\", agent2)\n",
    "workflow.add_node(\"judge\", judge_summary)\n",
    "\n",
    "# Define flow\n",
    "workflow.set_entry_point(\"agent1\")\n",
    "workflow.add_conditional_edges(\"agent1\", should_continue, {\n",
    "    \"continue\": \"agent2\",\n",
    "    \"end\": \"judge\"\n",
    "})\n",
    "workflow.add_conditional_edges(\"agent2\", should_continue, {\n",
    "    \"continue\": \"agent1\", \n",
    "    \"end\": \"judge\"\n",
    "})\n",
    "workflow.add_edge(\"judge\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b995ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: Should AI development be accelerated or slowed down?\n",
      "==================================================\n",
      "\n",
      "Ahmad El Sharaa: As a former diplomat with experience in international relations, I believe that AI development should be carefully accelerated with a focus on ethical considerations and potential consequences. It is crucial to strike a balance between advancing technology and ensuring that it is used for the betterment of society without causing harm or disruption.\n",
      "\n",
      "I understand the potential benefits of accelerating AI development, such as advancements in healthcare, transportation, and other industries. However, it is important to proceed with caution and prioritize ethical guidelines to mitigate potential risks and unintended consequences. As we move forward, it is essential to involve international cooperation and dialogue to address the global implications of AI development.\n",
      "\n",
      "Donald Trump: Listen, I get it. We all want to see progress and innovation, but we can't afford to rush into things without considering the potential risks. We need to prioritize the safety and ethical implications of AI development. It's not just about moving fast, it's about moving forward responsibly. Let's focus on finding the right balance between progress and caution.\n",
      "\n",
      "Ahmad El Sharaa: Ahmad El Sharaa: I completely agree, Mr. Trump. It's essential to prioritize the safety and ethical implications of AI development. Rushing into AI without considering the potential risks could have serious consequences. We need to find the right balance between progress and caution to ensure that AI is developed and utilized responsibly for the benefit of society.\n",
      "\n",
      "Donald Trump: Donald Trump: Absolutely, Ahmad. We need to make sure that we are not sacrificing safety and ethical considerations in the pursuit of progress. It's crucial to involve international cooperation and dialogue to address the global implications of AI development. We must proceed with caution and prioritize the well-being of our society as we move forward with AI advancement.\n",
      "\n",
      "Ahmad El Sharaa: Ahmad El Sharaa: Indeed, Mr. Trump. International cooperation and dialogue are crucial in addressing the global implications of AI development. By working together, we can ensure that AI is developed and utilized responsibly for the benefit of all. Thank you for emphasizing the importance of prioritizing the well-being of our society as we advance AI technology.\n",
      "\n",
      "Donald Trump: Donald Trump: Absolutely, Ahmad. We need to make sure that we are not sacrificing safety and ethical considerations in the pursuit of progress. It's crucial to involve international cooperation and dialogue to address the global implications of AI development. We must proceed with caution and prioritize the well-being of our society as we move forward with AI advancement.\n",
      "\n",
      "ðŸ”¨ VERDICT: In this debate, both Ahmad El Sharaa and Donald Trump demonstrated a clear understanding of the importance of balancing AI development with ethical considerations and potential risks. They both emphasized the need for caution and the prioritization of the well-being of society as AI technology advances. However, Ahmad El Sharaa's background as a former diplomat with experience in international relations provided a more nuanced and comprehensive perspective on the global implications of AI development. Therefore, Ahmad El Sharaa won this debate due to his well-reasoned arguments and emphasis on international cooperation and dialogue in addressing the challenges of AI advancement.\n"
     ]
    }
   ],
   "source": [
    "# Run the debate\n",
    "print(f\"TOPIC: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [],\n",
    "    \"turn_count\": 0,\n",
    "    \"max_turns\": 6,  # 3 rounds each\n",
    "    \"current_speaker\": \"agent1\"\n",
    "}\n",
    "\n",
    "result = graph.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f244b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f15298e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a04db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156a625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading logs from: work/app/logs/app.log\n",
      "\n",
      "2025-11-12 13:30:34,864 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:30:34,867 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:32:36,106 [httpx] [INFO] HTTP Request: GET https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct?expand=inferenceProviderMapping \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:32:40,158 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:32:40,160 [app.model_interface.llama_debator] [ERROR] Failed to create character: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "2025-11-12 13:34:18,644 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:34:18,647 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:34:27,771 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:34:27,773 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 13:34:36,327 [httpx] [INFO] HTTP Request: GET https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct?expand=inferenceProviderMapping \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:34:41,035 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:34:51,749 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:34:57,990 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:35:08,430 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 13:35:08,431 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:28:59,903 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:28:59,904 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:29:44,027 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:29:44,030 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:29:49,065 [httpx] [INFO] HTTP Request: GET https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct?expand=inferenceProviderMapping \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:29:50,489 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:29:50,490 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:30:12,667 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:30:12,670 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:31:03,883 [app.model_interface.llama_debator] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:31:03,885 [app.debate] [INFO] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:31:05,879 [httpx] [INFO] HTTP Request: GET https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct?expand=inferenceProviderMapping \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:31:06,983 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:31:06,983 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:31:33,301 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:31:33,301 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:31:47,943 [httpx] [INFO] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:31:47,943 [app.model_interface.llama_debator] [ERROR] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:33:04,848 [app.model_interface.llama_debator] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:33:04,850 [app.debate] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:34:17,513 [app.model_interface.llama_debator] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:34:17,516 [app.debate] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:35:20,804 [app.model_interface.llama_debator] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:35:20,807 [app.debate] [INFO] [logging:setup_logging] Logging initialized. Writing to /app/logs/app.log\n",
      "2025-11-12 15:35:23,194 [httpx] [INFO] [_client:_send_single_request] HTTP Request: GET https://huggingface.co/api/models/meta-llama/Meta-Llama-3-8B-Instruct?expand=inferenceProviderMapping \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:35:24,419 [httpx] [INFO] [_client:_send_single_request] HTTP Request: POST https://router.huggingface.co/novita/v3/openai/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-12 15:35:24,420 [app.model_interface.llama_debator] [INFO] [llama_debator:create_character_from_description] Raw response from model: Please provide the description of the character you'd like to create. \n",
      "\n",
      "(Note: Please format your input as if you were writing a short paragraph about the character.)\n",
      "2025-11-12 15:35:24,420 [app.model_interface.llama_debator] [ERROR] [llama_debator:create_character_from_description] JSON decoding error: Expecting value: line 1 column 1 (char 0)\n",
      "2025-11-12 15:35:24,420 [app.model_interface.llama_debator] [INFO] [llama_debator:create_character_from_description] Failed response content: Please provide the description of the character you'd like to create. \n",
      "\n",
      "(Note: Please format your input as if you were writing a short paragraph about the character.)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74bfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
